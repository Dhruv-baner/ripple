{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d9b64e",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Objective\n",
    "\n",
    "Extract insights from company business descriptions using NLP techniques:\n",
    "\n",
    "- **Topic modeling**: Cluster companies by business characteristics\n",
    "\n",
    "- **Semantic embeddings**: Capture business model similarities\n",
    "\n",
    "- **Keyword extraction**: Identify key business terms and risk indicators\n",
    "\n",
    "- **News sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3ca3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n",
      "\n",
      "Checking for additional NLP libraries...\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(\"\\nChecking for additional NLP libraries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7056a1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Metadata table not found or error: (sqlite3.OperationalError) near \"metadata\": syntax error\n",
      "[SQL: metadata]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "\n",
      "We need to extract business summaries from Notebook 01 first\n",
      "\n",
      "Dataset: 188 companies, 667 financial records\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "db_path = '../data/processed/company_data.db'\n",
    "engine = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "# Check if we have metadata with business summaries\n",
    "try:\n",
    "    df_metadata = pd.read_sql('metadata', engine)\n",
    "    has_summaries = 'business_summary' in df_metadata.columns\n",
    "    print(f\"âœ“ Metadata table found: {len(df_metadata)} companies\")\n",
    "    print(f\"Business summaries available: {has_summaries}\")\n",
    "    \n",
    "    if has_summaries:\n",
    "        # Check completeness\n",
    "        non_null = df_metadata['business_summary'].notna().sum()\n",
    "        print(f\"Companies with summaries: {non_null} ({non_null/len(df_metadata)*100:.1f}%)\")\n",
    "        \n",
    "        # Sample\n",
    "        print(\"\\nSample business summary:\")\n",
    "        sample = df_metadata[df_metadata['business_summary'].notna()].iloc[0]\n",
    "        print(f\"\\nCompany: {sample['ticker']}\")\n",
    "        print(f\"Summary: {sample['business_summary'][:300]}...\")\n",
    "    else:\n",
    "        print(\"âš  No business summaries in metadata table\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš  Metadata table not found or error: {e}\")\n",
    "    print(\"\\nWe need to extract business summaries from Notebook 01 first\")\n",
    "    has_summaries = False\n",
    "\n",
    "# Load companies and financials for reference\n",
    "df_companies = pd.read_sql('companies', engine)\n",
    "df_financials = pd.read_sql('financials', engine)\n",
    "\n",
    "print(f\"\\nDataset: {len(df_companies)} companies, {len(df_financials)} financial records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e46b79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Metadata loaded: 165 companies\n",
      "Business summaries available: 137\n",
      "\n",
      "Companies with text data: 137\n",
      "\n",
      "Sample summaries (first 200 chars):\n",
      "\n",
      "TSCO.L: Tesco PLC, together with its subsidiaries, operates as a grocery retailer in the United Kingdom, Republic of Ireland, the Czech Republic, Slovakia, and Hungary. It offers grocery products through its ...\n",
      "\n",
      "MKS.L: Marks and Spencer Group plc operates various retail stores. It operates through Fashion, Home & Beauty; Food; International; and Ocado segments. The company offers womenswear, menswear, lingerie, kids...\n",
      "\n",
      "NXT.L: NEXT plc engages in the retail of clothing, homeware, and beauty products in the United Kingdom, rest of Europe, the Middle East, Asia, and internationally. It operates through NEXT Online, NEXT Retai...\n"
     ]
    }
   ],
   "source": [
    "# Load metadata with business summaries\n",
    "df_metadata = pd.read_sql('company_metadata', engine)\n",
    "\n",
    "print(f\"âœ“ Metadata loaded: {len(df_metadata)} companies\")\n",
    "print(f\"Business summaries available: {df_metadata['business_summary'].notna().sum()}\")\n",
    "\n",
    "# Filter to companies with summaries\n",
    "df_text = df_metadata[df_metadata['business_summary'].notna()].copy()\n",
    "\n",
    "print(f\"\\nCompanies with text data: {len(df_text)}\")\n",
    "print(f\"\\nSample summaries (first 200 chars):\")\n",
    "for idx in range(min(3, len(df_text))):\n",
    "    print(f\"\\n{df_text.iloc[idx]['ticker']}: {df_text.iloc[idx]['business_summary'][:200]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98352f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Text statistics:\n",
      "Average summary length: 985 characters\n",
      "Average word count: 140 words\n",
      "Min/Max words: 35 / 796\n"
     ]
    }
   ],
   "source": [
    "# Basic text statistics\n",
    "df_text['summary_length'] = df_text['business_summary'].str.len()\n",
    "df_text['word_count'] = df_text['business_summary'].str.split().str.len()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Text statistics:\")\n",
    "print(f\"Average summary length: {df_text['summary_length'].mean():.0f} characters\")\n",
    "print(f\"Average word count: {df_text['word_count'].mean():.0f} words\")\n",
    "print(f\"Min/Max words: {df_text['word_count'].min():.0f} / {df_text['word_count'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b2f20",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Cleaning business summaries for NLP analysis:\n",
    "- Lowercase normalization\n",
    "- Special character removal\n",
    "- Extended stop word list (removing common corporate terms like \"plc\", \"operates\", \"provides\")\n",
    "\n",
    "Preparing text for topic modeling and keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7fa723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "\n",
      "âœ“ Text cleaned and preprocessed\n",
      "Stop words: 336 terms\n",
      "\n",
      "Sample cleaned text:\n",
      "tesco plc together with its subsidiaries operates as a grocery retailer in the united kingdom republic of ireland the czech republic slovakia and hungary it offers grocery products through its stores ...\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean business summaries for NLP analysis\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing text data...\\n\")\n",
    "\n",
    "df_text['summary_clean'] = df_text['business_summary'].apply(clean_text)\n",
    "\n",
    "# Create stop words list (extend with finance-specific common words)\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "finance_stopwords = [\n",
    "    'company', 'companies', 'business', 'operates', 'provides', \n",
    "    'offers', 'include', 'includes', 'services', 'products',\n",
    "    'plc', 'ltd', 'limited', 'group', 'together', 'subsidiaries',\n",
    "    'operations', 'operates', 'engaged', 'engage', 'engages'\n",
    "]\n",
    "\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(finance_stopwords))\n",
    "\n",
    "print(f\"âœ“ Text cleaned and preprocessed\")\n",
    "print(f\"Stop words: {len(stop_words)} terms\")\n",
    "\n",
    "print(\"\\nSample cleaned text:\")\n",
    "print(df_text.iloc[0]['summary_clean'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28498a2e",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "Using Latent Dirichlet Allocation (LDA) to discover latent business categories from descriptions:\n",
    "\n",
    "- **Method**: TF-IDF vectorization + LDA clustering\n",
    "\n",
    "- **Parameters**: 5 topics, bigrams included, finance stopwords removed\n",
    "\n",
    "- **Output**: Each company assigned to dominant business theme\n",
    "\n",
    "Topics reveal natural groupings beyond simple sector labels (e.g., \"digital platform companies\" vs \"physical infrastructure\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15a3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running topic modeling (Latent Dirichlet Allocation)...\n",
      "\n",
      "âœ“ TF-IDF matrix created: (137, 500)\n",
      "âœ“ LDA model trained with 5 topics\n",
      "\n",
      "============================================================\n",
      "DISCOVERED BUSINESS TOPICS\n",
      "============================================================\n",
      "\n",
      "Topic 0: fund, invests, equity, gas, firm, electricity, investments, oil, markets, games\n",
      "\n",
      "Topic 1: segment, accessories, systems, home, household, care, beverages, air, beauty, water\n",
      "\n",
      "Topic 2: data, housing, homes, uk, content, london, supplies, property, advertising, real\n",
      "\n",
      "Topic 3: solutions, hotels, drinks, systems, inn, development, internationally, power, london united, medicines\n",
      "\n",
      "Topic 4: banking, management, stores, retail, europe, financial, insurance, founded, america, asia\n",
      "\n",
      "============================================================\n",
      "Companies by topic:\n",
      "\n",
      "Topic 0 (35 companies): BNZL.L, BP.L, SHEL.L, SSE.L, NG.L, III.L, GLEN.L, RIO.L, ANTO.L, RCP.L...\n",
      "\n",
      "Topic 1 (10 companies): ABF.L, AUTO.L, HLMA.L, RR.L, BA.L, ULVR.L, RKT.L, PNN.L, MCB.L, WIX.L...\n",
      "\n",
      "Topic 2 (20 companies): RMV.L, FCIT.L, PSN.L, ITV.L, WPP.L, UTG.L, DLN.L, BKG.L, BWY.L, MGNS.L...\n",
      "\n",
      "Topic 3 (21 companies): OCDO.L, MNDI.L, SMIN.L, WTB.L, AZN.L, GSK.L, AAL.L, CCH.L, FEVR.L, MTO.L...\n",
      "\n",
      "Topic 4 (51 companies): TSCO.L, MKS.L, NXT.L, SBRY.L, KGF.L, JD.L, FRAS.L, SGE.L, IMB.L, DGE.L...\n"
     ]
    }
   ],
   "source": [
    "# Topic modeling with LDA\n",
    "print(\"Running topic modeling (Latent Dirichlet Allocation)...\\n\")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 2)  # Include bigrams\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df_text['summary_clean'])\n",
    "\n",
    "print(f\"âœ“ TF-IDF matrix created: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Train LDA model\n",
    "n_topics = 5  # 5 business categories\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=20\n",
    ")\n",
    "\n",
    "lda_topics = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"âœ“ LDA model trained with {n_topics} topics\\n\")\n",
    "\n",
    "# Display top words per topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DISCOVERED BUSINESS TOPICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_indices = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "    print(f\"\\nTopic {topic_idx}: {', '.join(top_words)}\")\n",
    "\n",
    "# Assign dominant topic to each company\n",
    "df_text['dominant_topic'] = lda_topics.argmax(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Companies by topic:\")\n",
    "for topic in range(n_topics):\n",
    "    companies = df_text[df_text['dominant_topic'] == topic]['ticker'].tolist()\n",
    "    print(f\"\\nTopic {topic} ({len(companies)} companies): {', '.join(companies[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a6929",
   "metadata": {},
   "source": [
    "### **Keyword Extraction**\n",
    "\n",
    "Creating binary features based on business characteristics mentioned in descriptions:\n",
    "\n",
    "**Categories extracted:**\n",
    "\n",
    "- **Digital**: Online platforms, software, tech-focused companies\n",
    "\n",
    "- **International**: Global operations and export-oriented businesses\n",
    "\n",
    "- **Restructuring**: Companies undergoing transformation (potential distress signal)\n",
    "\n",
    "- **Innovation**: R&D-intensive, patent-focused businesses\n",
    "\n",
    "- **Retail/Manufacturing/Services/Infrastructure**: Core business model types\n",
    "\n",
    "These keywords become features for ML models and provide interpretable business context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36586d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting business keywords...\n",
      "\n",
      "Keyword flags created:\n",
      "  has_digital              :  65 companies ( 47.4%)\n",
      "  has_international        :  65 companies ( 47.4%)\n",
      "  has_restructuring        :   3 companies (  2.2%)\n",
      "  has_innovation           :  72 companies ( 52.6%)\n",
      "  has_retail_physical      :  54 companies ( 39.4%)\n",
      "  has_manufacturing        :  29 companies ( 21.2%)\n",
      "  has_services             :   8 companies (  5.8%)\n",
      "  has_infrastructure       :  48 companies ( 35.0%)\n",
      "\n",
      "============================================================\n",
      "Most common business terms:\n",
      "  united              : 240 mentions\n",
      "  kingdom             : 215 mentions\n",
      "  founded             :  86 mentions\n",
      "  london              :  86 mentions\n",
      "  management          :  81 mentions\n",
      "  solutions           :  76 mentions\n",
      "  segment             :  74 mentions\n",
      "  europe              :  71 mentions\n",
      "  headquartered       :  69 mentions\n",
      "  based               :  67 mentions\n",
      "  segments            :  62 mentions\n",
      "  retail              :  56 mentions\n",
      "  including           :  56 mentions\n",
      "  markets             :  56 mentions\n",
      "  investment          :  53 mentions\n"
     ]
    }
   ],
   "source": [
    "# Extract meaningful keywords and create binary features\n",
    "print(\"Extracting business keywords...\\n\")\n",
    "\n",
    "# Define keyword categories\n",
    "keyword_categories = {\n",
    "    'digital': ['digital', 'online', 'platform', 'software', 'technology', 'data', 'cloud', 'app'],\n",
    "    'international': ['international', 'global', 'worldwide', 'export', 'overseas', 'multinational'],\n",
    "    'restructuring': ['restructuring', 'transformation', 'turnaround', 'reorganization', 'streamline'],\n",
    "    'innovation': ['innovation', 'research', 'development', 'rd', 'patent', 'innovative'],\n",
    "    'retail_physical': ['store', 'stores', 'shop', 'retail', 'branch', 'outlet'],\n",
    "    'manufacturing': ['manufacturing', 'production', 'factory', 'plant', 'assembly'],\n",
    "    'services': ['consulting', 'advisory', 'professional services', 'outsourcing'],\n",
    "    'infrastructure': ['infrastructure', 'network', 'facilities', 'assets', 'property']\n",
    "}\n",
    "\n",
    "# Create binary flags for each category\n",
    "for category, keywords in keyword_categories.items():\n",
    "    pattern = '|'.join(keywords)\n",
    "    df_text[f'has_{category}'] = df_text['summary_clean'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "\n",
    "print(\"Keyword flags created:\")\n",
    "keyword_cols = [col for col in df_text.columns if col.startswith('has_')]\n",
    "for col in keyword_cols:\n",
    "    count = df_text[col].sum()\n",
    "    print(f\"  {col:25s}: {count:3d} companies ({count/len(df_text)*100:5.1f}%)\")\n",
    "\n",
    "# Extract most common individual terms (excluding stopwords)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Most common business terms:\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "all_words = ' '.join(df_text['summary_clean']).split()\n",
    "filtered_words = [w for w in all_words if w not in stop_words and len(w) > 3]\n",
    "word_counts = Counter(filtered_words).most_common(20)\n",
    "\n",
    "for word, count in word_counts[:15]:\n",
    "    print(f\"  {word:20s}: {count:3d} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5b65e",
   "metadata": {},
   "source": [
    "### **Semantic Similarity Analysis**\n",
    "\n",
    "Using TF-IDF embeddings to measure business model similarity:\n",
    "\n",
    "**Cosine similarity**: Measures how similar two business descriptions are (0 = completely different, 1 = identical)\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "- **Peer identification**: Find companies with similar business models for comparison\n",
    "\n",
    "- **Risk contagion**: Companies similar to distressed peers may face similar pressures\n",
    "\n",
    "- **Feature engineering**: Similarity to distressed companies as predictive signal\n",
    "\n",
    "**Method**: Each company represented as 500-dimensional TF-IDF vector, cosine similarity computed between all pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec9279f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating semantic embeddings...\n",
      "\n",
      "âœ“ Using TF-IDF embeddings: (137, 500)\n",
      "âœ“ Similarity matrix computed: (137, 137)\n",
      "Semantic similarity analysis complete\n",
      "\n",
      "============================================================\n",
      "BUSINESS MODEL SIMILARITY (Sample Companies)\n",
      "============================================================\n",
      "\n",
      "TSCO.L:\n",
      "  Most similar to: CURY.L, DFS.L, HFD.L\n",
      "\n",
      "BP.L:\n",
      "  Most similar to: SHEL.L, ENQ.L, GLEN.L\n",
      "\n",
      "AZN.L:\n",
      "  Most similar to: GSK.L, CTEC.L, SXS.L\n",
      "\n",
      "LSEG.L:\n",
      "  Most similar to: IGG.L, RDT.L, PCTN.L\n",
      "\n",
      "RR.L:\n",
      "  Most similar to: XPP.L, BA.L, SMIN.L\n"
     ]
    }
   ],
   "source": [
    "# Create semantic embeddings and find similar companies\n",
    "print(\"\\nCreating semantic embeddings...\\n\")\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Use TF-IDF vectors as semantic embeddings\n",
    "# (In production, could use more sophisticated models like sentence-transformers)\n",
    "\n",
    "print(f\"âœ“ Using TF-IDF embeddings: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Compute pairwise similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(f\"âœ“ Similarity matrix computed: {similarity_matrix.shape}\")\n",
    "\n",
    "# For each company, find most similar peers\n",
    "df_text['similar_companies'] = None\n",
    "\n",
    "for idx, ticker in enumerate(df_text['ticker'].values):\n",
    "    # Get similarity scores for this company\n",
    "    sim_scores = similarity_matrix[idx]\n",
    "    \n",
    "    # Get indices of top 3 most similar (excluding itself)\n",
    "    similar_indices = sim_scores.argsort()[-4:-1][::-1]  # Top 3 (excluding self)\n",
    "    \n",
    "    similar_tickers = df_text.iloc[similar_indices]['ticker'].values\n",
    "    df_text.at[df_text.index[idx], 'similar_companies'] = ', '.join(similar_tickers)\n",
    "\n",
    "print(\"Semantic similarity analysis complete\\n\")\n",
    "\n",
    "# Show examples\n",
    "print(\"=\"*60)\n",
    "print(\"BUSINESS MODEL SIMILARITY (Sample Companies)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_companies = ['TSCO.L', 'BP.L', 'AZN.L', 'LSEG.L', 'RR.L']\n",
    "sample_companies = [t for t in sample_companies if t in df_text['ticker'].values]\n",
    "\n",
    "for ticker in sample_companies[:5]:\n",
    "    if ticker in df_text['ticker'].values:\n",
    "        row = df_text[df_text['ticker'] == ticker].iloc[0]\n",
    "        print(f\"\\n{ticker}:\")\n",
    "        print(f\"  Most similar to: {row['similar_companies']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5534cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Similarity to distressed companies (potential risk signal):\n",
      "  Mean similarity: 0.059\n",
      "  Std deviation: 0.040\n",
      "\n",
      "Companies most similar to distressed peers:\n",
      "ticker  similarity_to_distressed\n",
      " BME.L                  0.208364\n",
      "WIZZ.L                  0.200905\n",
      " WIX.L                  0.196253\n",
      "  RR.L                  0.194744\n",
      " CAR.L                  0.189208\n"
     ]
    }
   ],
   "source": [
    "# Calculate average similarity to distressed companies (risk feature)\n",
    "# Load distress flags\n",
    "df_financials_latest = df_financials.sort_values('date').groupby('ticker').tail(1)\n",
    "distressed_tickers = df_financials_latest[df_financials_latest['in_distress']]['ticker'].values\n",
    "\n",
    "df_text['similarity_to_distressed'] = 0.0\n",
    "\n",
    "for idx, ticker in enumerate(df_text['ticker'].values):\n",
    "    # Find distressed companies in our text dataset\n",
    "    distressed_indices = [i for i, t in enumerate(df_text['ticker'].values) if t in distressed_tickers]\n",
    "    \n",
    "    if len(distressed_indices) > 0:\n",
    "        # Average similarity to all distressed companies\n",
    "        avg_sim = similarity_matrix[idx, distressed_indices].mean()\n",
    "        df_text.at[df_text.index[idx], 'similarity_to_distressed'] = avg_sim\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Similarity to distressed companies (potential risk signal):\")\n",
    "print(f\"  Mean similarity: {df_text['similarity_to_distressed'].mean():.3f}\")\n",
    "print(f\"  Std deviation: {df_text['similarity_to_distressed'].std():.3f}\")\n",
    "\n",
    "print(\"\\nCompanies most similar to distressed peers:\")\n",
    "top_risk = df_text.nlargest(5, 'similarity_to_distressed')[['ticker', 'similarity_to_distressed']]\n",
    "print(top_risk.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb924a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREPARING NLP FEATURES FOR ML\n",
      "============================================================\n",
      "\n",
      "NLP features created: 12 features\n",
      "Companies covered: 137\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36408\\1210370913.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m\\nNLP features created: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m features\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mCompanies covered: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_nlp_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Merge with existing companies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdf_companies_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_companies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_nlp_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ticker'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Fill missing values for companies without text data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeyword_cols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ticker'"
     ]
    }
   ],
   "source": [
    "# Prepare NLP features for modeling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING NLP FEATURES FOR ML\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select NLP features to save\n",
    "nlp_features = ['ticker'] + keyword_cols + [\n",
    "    'summary_length', \n",
    "    'word_count',\n",
    "    'similarity_to_distressed',\n",
    "    'dominant_topic'\n",
    "]\n",
    "\n",
    "df_nlp_features = df_text[nlp_features].copy()\n",
    "\n",
    "print(f\"\\nNLP features created: {len(nlp_features)-1} features\")\n",
    "print(f\"Companies covered: {len(df_nlp_features)}\")\n",
    "\n",
    "# Merge with existing companies\n",
    "df_companies_full = df_companies.merge(df_nlp_features, on='ticker', how='left')\n",
    "\n",
    "# Fill missing values for companies without text data\n",
    "for col in keyword_cols:\n",
    "    df_companies_full[col].fillna(0, inplace=True)\n",
    "    \n",
    "df_companies_full['similarity_to_distressed'].fillna(df_companies_full['similarity_to_distressed'].mean(), inplace=True)\n",
    "df_companies_full['dominant_topic'].fillna(-1, inplace=True)  # -1 = no text data\n",
    "\n",
    "print(f\"\\nTotal companies after merge: {len(df_companies_full)}\")\n",
    "print(f\"Companies with NLP features: {df_companies_full['has_digital'].notna().sum()}\")\n",
    "\n",
    "# Save to database\n",
    "df_nlp_features.to_sql('nlp_features', engine, if_exists='replace', index=False)\n",
    "print(f\"\\nâœ“ Saved: nlp_features ({len(df_nlp_features)} companies)\")\n",
    "\n",
    "df_companies_full.to_sql('companies_enriched', engine, if_exists='replace', index=False)\n",
    "print(f\"âœ“ Saved: companies_enriched ({len(df_companies_full)} companies with NLP)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NLP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFeatures created:\")\n",
    "print(f\"  â€¢ {len(keyword_cols)} keyword flags (digital, innovation, etc.)\")\n",
    "print(f\"  â€¢ 2 text statistics (length, word count)\")\n",
    "print(f\"  â€¢ 1 similarity metric (to distressed companies)\")\n",
    "print(f\"  â€¢ 1 topic assignment (5 business categories)\")\n",
    "print(f\"\\nKey finding: Similarity to distressed companies shows predictive signal\")\n",
    "print(f\"  - Wizz Air (distressed): 0.20 similarity\")\n",
    "print(f\"  - Rolls-Royce (was distressed): 0.19 similarity\")\n",
    "print(f\"  - Mean baseline: 0.06 similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7331716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in df_text:\n",
      "['ticker', 'market_cap', 'employees', 'industry', 'country', 'audit_risk', 'board_risk', 'overall_risk', 'business_summary', 'summary_length', 'word_count', 'summary_clean', 'dominant_topic', 'has_digital', 'has_international', 'has_restructuring', 'has_innovation', 'has_retail_physical', 'has_manufacturing', 'has_services', 'has_infrastructure', 'similar_companies', 'similarity_to_distressed']\n"
     ]
    }
   ],
   "source": [
    "# Check column names\n",
    "print(\"Available columns in df_text:\")\n",
    "print(df_text.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2410a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREPARING NLP FEATURES FOR ML\n",
      "============================================================\n",
      "Keyword columns found: ['has_digital', 'has_international', 'has_restructuring', 'has_innovation', 'has_retail_physical', 'has_manufacturing', 'has_services', 'has_infrastructure']\n",
      "\n",
      "NLP features created: 12 features\n",
      "Companies covered: 137\n",
      "\n",
      "âœ“ Saved: nlp_features (137 companies, 13 columns)\n",
      "\n",
      "============================================================\n",
      "NLP SUMMARY\n",
      "============================================================\n",
      "\n",
      "Features created:\n",
      "  â€¢ 8 keyword flags\n",
      "  â€¢ 2 text statistics\n",
      "  â€¢ 1 similarity metric\n",
      "  â€¢ 1 topic assignment\n",
      "\n",
      "Key finding: Similarity to distressed companies = 0.059 (mean)\n",
      "Companies with high risk similarity (>0.15): 7\n"
     ]
    }
   ],
   "source": [
    "# Prepare NLP features for modeling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING NLP FEATURES FOR ML\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define keyword columns\n",
    "keyword_cols = [col for col in df_text.columns if col.startswith('has_')]\n",
    "\n",
    "print(f\"Keyword columns found: {keyword_cols}\")\n",
    "\n",
    "# Select NLP features to save\n",
    "nlp_features = ['ticker'] + keyword_cols + [\n",
    "    'summary_length', \n",
    "    'word_count',\n",
    "    'similarity_to_distressed',\n",
    "    'dominant_topic'\n",
    "]\n",
    "\n",
    "# Verify all columns exist\n",
    "missing = [col for col in nlp_features if col not in df_text.columns]\n",
    "if missing:\n",
    "    print(f\"Warning: Missing columns: {missing}\")\n",
    "    nlp_features = [col for col in nlp_features if col in df_text.columns]\n",
    "\n",
    "df_nlp_features = df_text[nlp_features].copy()\n",
    "\n",
    "print(f\"\\nNLP features created: {len(nlp_features)-1} features\")\n",
    "print(f\"Companies covered: {len(df_nlp_features)}\")\n",
    "\n",
    "# Save to database\n",
    "df_nlp_features.to_sql('nlp_features', engine, if_exists='replace', index=False)\n",
    "print(f\"\\nâœ“ Saved: nlp_features ({len(df_nlp_features)} companies, {len(nlp_features)} columns)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NLP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFeatures created:\")\n",
    "print(f\"  â€¢ {len(keyword_cols)} keyword flags\")\n",
    "print(f\"  â€¢ 2 text statistics\")\n",
    "print(f\"  â€¢ 1 similarity metric\")\n",
    "print(f\"  â€¢ 1 topic assignment\")\n",
    "print(f\"\\nKey finding: Similarity to distressed companies = {df_text['similarity_to_distressed'].mean():.3f} (mean)\")\n",
    "print(f\"Companies with high risk similarity (>0.15): {(df_text['similarity_to_distressed'] > 0.15).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f541d36",
   "metadata": {},
   "source": [
    "### **NLP Analysis Complete**\n",
    "\n",
    "Successfully extracted 13 text-based features from 137 company business descriptions (73% coverage).\n",
    "\n",
    "#### **Feature Categories**\n",
    "\n",
    "- **Business model keywords (8)**: Digital, international, restructuring, innovation, retail, manufacturing, services, infrastructure\n",
    "\n",
    "- **Text characteristics (2)**: Summary length, word count  \n",
    "\n",
    "- **Semantic features (2)**: Topic assignment, similarity to distressed companies\n",
    "\n",
    "- **Coverage**: 137/188 companies (73%) have text data\n",
    "\n",
    "#### **Key Insights**\n",
    "\n",
    "ðŸŽ¯ **Risk signal identified**: 7 companies show high similarity (>0.15) to distressed business models\n",
    "- Wizz Air: 0.20 (currently distressed)\n",
    "- Rolls-Royce: 0.19 (post-COVID recovery)\n",
    "- Mean baseline: 0.06\n",
    "\n",
    "ðŸ“Š **Business characteristics**:\n",
    "- 53% innovation/R&D focused\n",
    "- 47% digital/tech-oriented  \n",
    "- 47% international operations\n",
    "- 39% physical retail presence\n",
    "\n",
    "#### **Value for ML Modeling**\n",
    "Text features provide complementary signals to financial metrics:\n",
    "\n",
    "- Business model context (digital vs physical)\n",
    "\n",
    "- Strategic positioning (innovation, international expansion)\n",
    "\n",
    "- Semantic risk (similarity to distressed peers)\n",
    "\n",
    "**Next**: Integrate NLP features with financial/temporal features in ML models (NB05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
